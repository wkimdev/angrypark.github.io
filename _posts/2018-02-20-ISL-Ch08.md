---
title: "[ISL] Ch8. "  
layout: post  
date: 2018-02-14 18:00  
image: /assets/images/170720/background.jpeg  
headerImage: false  
tag:  
- machine learning  
- statistical learning  
- ISL  
category: blog  
author: angrypark

description: YBIGTA에서 진행하는 ISL(Introduction to Statistical Learning) Chapter 7 정리입니다.

---

<span style="color:#7C7877; font-family: 'Apple SD Gothic Neo'; font-weight:200">

이번 시간에는 ISL(Introduction to Statistical Learning) 7장을 리뷰해보았습니다. 7장에서는 Moving Beyond Linearity, 선형성을 확장하고자 시도했던 여러 모델들을 배우는데요. 선형모델들의 단순 확장인 step function, polynomial regression 뿐만 아니라 splone, local regression, generalized additive model들도 다룹니다.

## 목차  
- [7.1. Polynomial Regression](#71-polynomial-regression)  
- [7.2. Step Function](#72-step-function)  
- [7.3. Basis Function](#73-base-function)  
- [7.4. Regression Splines](#74-regression-splines)  
- [7.5. Smoothing Splines](#75-smoothing-splines)  
- [7.6. Local Regression](#76-local-regression)
-	[7.7. Generalized Additive Models](#77-generalized-additive-models)

---


## 7.1. Polynomial Regression  
역사적으로 선형회귀를 비선형적 모델로 확장시킨 가장 대표적인 회귀는 다음과 같은 선형 모델을

$$
y_i = \beta_0 + \beta_1 x_i+\epsilon_i
$$

우리가 알고 있는 다항식 형식으로 대체하는 것입니다.

$$
y_i = \beta_0 + \beta_1 x_i+\beta_2 x_i^2+\beta_3 x_i^3+...+\beta_d x_i^d+\epsilon_i
$$

단순선형회귀와 마찬가지로 최소제곱법을 활용하여 쉽게 추정할 수도 있지만, d(차원)이 커질수록 지나치게 flexible해져서 이상한 결과를 낳을 수 있습니다.

![1](/assets/2018-02-14/1.PNG)  

위 그림에서 점선으로 된 곡선은 표준오차 곡선들입니다. 그렇다면 특정 값 $$x_0$$에 대해 어떻게 분산을 구해서 신뢰구간을 그린 것일까요? 특정 값 $$x_0$$에서의 적합의 분산 $$Var \hat f(x_0)$$은 **$$\hat \beta_j$$ 각각에 대한 분산 추정치** 들과 **계수 추정치** 들 사이의 공분산을 가지고 구할 수 있다고 합니다. 오른쪽 그림은 polynomial regression을 설명변수로 logistic regression을 한 결과입니다. 그 결과 age에 따라서 신뢰구간의 길이가 상당히 많이 차이가 나는 것을 확인하실 수 있습니다. 그 이유는 표본은 크지만 고소득자의 비율(3000명 중 79명)이 매우 작기 때문입니다.

---

## 7.2. Step Function
Step function은 비선형성을 보이는 $$X$$의 범위를 여러 개의 bin으로 분할합니다. 즉, 연속적인 변수를 **ordered categorical variable** 로 바꾸는 것입니다. 순서가 있는 범주형 변수로 말이죠. 즉, bin의 갯수가 $$K$$개라면 이를 활용하여 $$K+1$$개의 새로운 변수를 만듭니다. 그 결과 다음과 같은 회귀식이 생성됩니다.
$$
y_i=\beta_0+\beta_1C_1(x_i)+\beta_2C_2(x_i)+...+\beta_KC_K(x_i)+\epsilon_i
$$
물론 이렇게 하면 주어진 $$X$$에 대해 위의 식에서 기껏해야 1개가 0이 아닌 값이 나옵니다. 그렇다면, 설명변수의 bin들을 나누는 그 기준은 어떻게 설정해야 할까요?

![2](/assets/2018-02-14/2.png)
위에서 왼쪽 그림은 그 bin에 따라 age가 증가함을 보여주지 못합니다. 근데 또 epidemiology 분야에서 자주 사용된다고 하네요. 예를 들어 단순 나이를 연속형을 넣지 않고, 5년 단위로 끊어서 그룹화하는 형식이 있다고 합니다.

> 예상되는 단점 : 연속적이지 않아 그 경계를 반영 못함
> 장점 : segmentation하는 효과가 있을듯

---
## 7.3. Basis Function
결국 다항식 모델은 어떤 $$x_i$$에 대하여 이를 변수 변환한 특별한 케이스라고 볼 수 있습니다. 이러한 형식으로 다양한 base function들을 활용하여 변환의 family를 가질 수 있습니다.
$$
y_i=\beta_0+\beta_1b_1(x_i)+\beta_2b_2(x_i)+...+\beta_Kb_1(x_K)+\epsilon_i
$$

이는 지금까지 소개했던 모델들에 적용할 수 있습니다.
- polynomial regression : $$b_j(x_i)=x_i^j$$
- step function : $$b_j(x_i)=I(c_j\leq x_i \leq c_{j+1})$$

다른 방법들도 있습니다. Wavelets 또는 푸리에 급수 등을 예로 들 수 있는데요, 이제는 이런 base function으로 자주 사용되는 regression splines에 대해 알아보도록 하겠습니다.

> 질문 : base function 모아보기 + 언제 어떤 걸 쓸지

---
## 7.4. Regression Splines
![3](/assets/2018-02-14/3.png)

### 7.4.1. Piecewise Polynomial Regression
위에서 소개했던 polynomial regression을 $$X$$의 범위를 구분하여 각 범위에 다 다른 저차원 다항식을 적합하는 형식입니다. 그 결과 다음과 같은 형태를 가지게 됩니다.
$$
y_i = \beta_{01}+\beta_{11}x_i+\beta_{21}x_i^2+\beta_{31}x_i^3 (x_i<c)
$$
$$
y_i = \beta_{02}+\beta_{12}x_i+\beta_{22}x_i^2+\beta_{32}x_i^3 (x_i\geq c)
$$
위의 그림에서 왼쪽 위와 같은 형식입니다.

### 7.4.2. Constraints and Splines
Piecewise polynomial regression은 적합곡선이 너무 유연하다는 단점이 있습니다. 이를 해결하기 위해 2가지의 제한조건을 추가하였는데 그는 다음과 같습니다.
1. 범위의 경계에서 smooth해야 합니다.
2. 범위의 경계에서 연결되어야 합니다.

- 질문 : 그래서 뭐가 좋은거야..?

---

## 7.5. Smoothing Splines
[7.4절](#74-regression-splines)에서 regression splines를 다룰 때, 매듭(knot)을 지정하고 base function을 도출한 다음 최소제곱을 사용하여 spline 계수를 추정하였습니다. 이 때

---

## 7.6. Local Regression
 **Local regression** 은 유연한 비선형함수들을 적합하는 다른 기법으로, 목표점 $$x_0$$에서 그 주변의 train data들만 사용한다는 특징이 있습니다. 이는 시간과 같이 국소적으로 변하는 변수(책에서는 varying coefficient model)들에 적용하면 유용합니다. 이런 국소적 방법과 유사한 것이 앞에서 배웠던 k-Nearest-Neighbors 입니다. 고차원이면 안좋대요~

---

## 7.7. Generalized Additive Models
앞서 배웠던 모든 모델들은 모두 하나의 설명변수 $$X$$를 유연하게 바꿔서 $$Y$$를 예측하는 모델들이었습니다. 단순히 변수 변환이라고 생각할 수 있죠. 이 장에서는 여러 개의 설명변수들 $$X_1$$, $$X_2$$, ..., $$X_p$$를 기반으로 $$Y$$를 예측하는 문제를 소개합니다. 결국 단순선형회귀 $$\rightarrow$$ 다중선형회귀와 비슷한 전개라고 생각하시면 됩니다. **Generalized Additive Models** (이하 GAM)은 additivity를 유지하면서 각 변수의 비선형함수를 허용하여 선형 모델들을 확장합니다.

### 7.7.1. GAMs in Regression
식으로 보면 금방 이해되실 겁니다. 아래의 다중선형모델을
### 7.7.2. GAMs in Classification

> **GAMs의 장점과 단점**
>- GAMs는 $$X_j$$ 각각에 비선형 함수 $$f_j$$를 적합할 수 있어 비선형 관계를 자동으로 모델링할 수 있습니다. 각 변수에 대해 다 할 필요가 없다는 거죠.
>- $$Y$$를 더 정확하게 예측할 가능성이 있다고 합니다.
>- 이 모델은 additivity를 만족하기 때문에 **$$Y$$에 대한 $$X_j$$ 각각의 영향** 을 다른 변수들은 모두 고정하고서 개별적으로 확인할 수 있습니다. 즉, 추론에 적용하기 매우 좋습니다.
>- 변수 $$X_j$$에 대한 함수 $$f_j$$의 smoothness를 자유도로 요약할 수 있습니다.
>- 한계는 모델의 additivity입니다. 많은 변수들이 있을 경우 interaction effect를 놓칠 수 있습니다. 하지만, 수동으로 interaction effect를 후에 넣음으로서 이를 완화시킬 수 있습니다.

---
